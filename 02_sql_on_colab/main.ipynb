{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6n7RrUqHhPlL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jSvT58AhPlM"
      },
      "source": [
        "## SQL Hepler Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vVq1qSQKhPlO"
      },
      "outputs": [],
      "source": [
        "def pd_to_sqlDB(input_df: pd.DataFrame,\n",
        "                table_name: str,\n",
        "                db_name: str = 'default.db') -> None:\n",
        "\n",
        "    '''Take a Pandas dataframe `input_df` and upload it to `table_name` SQLITE table\n",
        "\n",
        "    Args:\n",
        "        input_df (pd.DataFrame): Dataframe containing data to upload to SQLITE\n",
        "        table_name (str): Name of the SQLITE table to upload to\n",
        "        db_name (str, optional): Name of the SQLITE Database in which the table is created.\n",
        "                                 Defaults to 'default.db'.\n",
        "    '''\n",
        "\n",
        "    # Step 1: Setup local logging\n",
        "    import logging\n",
        "    logging.basicConfig(level=logging.INFO,\n",
        "                        format='%(asctime)s %(levelname)s: %(message)s',\n",
        "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # Step 2: Find columns in the dataframe\n",
        "    cols = input_df.columns\n",
        "    cols_string = ','.join(cols)\n",
        "    val_wildcard_string = ','.join(['?'] * len(cols))\n",
        "\n",
        "    # Step 3: Connect to a DB file if it exists, else crete a new file\n",
        "    con = sqlite3.connect(db_name)\n",
        "    cur = con.cursor()\n",
        "    logging.info(f'SQL DB {db_name} created')\n",
        "\n",
        "    # Step 4: Create Table\n",
        "    sql_string = f\"\"\"CREATE TABLE {table_name} ({cols_string});\"\"\"\n",
        "    cur.execute(sql_string)\n",
        "    logging.info(f'SQL Table {table_name} created with {len(cols)} columns')\n",
        "\n",
        "    # Step 5: Upload the dataframe\n",
        "    rows_to_upload = input_df.to_dict(orient='split')['data']\n",
        "    sql_string = f\"\"\"INSERT INTO {table_name} ({cols_string}) VALUES ({val_wildcard_string});\"\"\"\n",
        "    cur.executemany(sql_string, rows_to_upload)\n",
        "    logging.info(f'{len(rows_to_upload)} rows uploaded to {table_name}')\n",
        "\n",
        "    # Step 6: Commit the changes and close the connection\n",
        "    con.commit()\n",
        "    con.close()\n",
        "\n",
        "\n",
        "def sql_query_to_pd(sql_query_string: str, db_name: str ='default.db') -> pd.DataFrame:\n",
        "    '''Execute an SQL query and return the results as a pandas dataframe\n",
        "\n",
        "    Args:\n",
        "        sql_query_string (str): SQL query string to execute\n",
        "        db_name (str, optional): Name of the SQLITE Database to execute the query in.\n",
        "                                 Defaults to 'default.db'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Results of the SQL query in a pandas dataframe\n",
        "    '''\n",
        "    # Step 1: Connect to the SQL DB\n",
        "    con = sqlite3.connect(db_name)\n",
        "\n",
        "    # Step 2: Execute the SQL query\n",
        "    cursor = con.execute(sql_query_string)\n",
        "\n",
        "    # Step 3: Fetch the data and column names\n",
        "    result_data = cursor.fetchall()\n",
        "    cols = [description[0] for description in cursor.description]\n",
        "\n",
        "    # Step 4: Close the connection\n",
        "    con.close()\n",
        "\n",
        "    # Step 5: Return as a dataframe\n",
        "    return pd.DataFrame(result_data, columns=cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrwwQ4oWhPlR"
      },
      "source": [
        "## Execute Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "FJPyqS4QhPlS",
        "outputId": "feb766e3-ec61-4aea-bce3-ea9d017ea2e5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'country_vaccinations.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1840149523.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 1: Read the csv file into a dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Dataset from https://www.kaggle.com/gpreda/covid-world-vaccination-progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'country_vaccinations.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Step 2: Upload the dataframe to a SQL Table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'country_vaccinations.csv'"
          ]
        }
      ],
      "source": [
        "# Step 1: Read the csv file into a dataframe\n",
        "# Dataset from https://www.kaggle.com/gpreda/covid-world-vaccination-progress\n",
        "input_df = pd.read_csv('country_vaccinations.csv')\n",
        "\n",
        "# Step 2: Upload the dataframe to a SQL Table\n",
        "pd_to_sqlDB(input_df,\n",
        "            table_name='country_vaccinations',\n",
        "            db_name='default.db')\n",
        "\n",
        "# Step 3: Write the SQL query in a string variable\n",
        "sql_query_string = \"\"\"\n",
        "    SELECT country, SUM(daily_vaccinations) as total_vaccinated\n",
        "    FROM country_vaccinations\n",
        "    WHERE daily_vaccinations IS NOT NULL\n",
        "    GROUP BY country\n",
        "    ORDER BY total_vaccinated DESC\n",
        "\"\"\"\n",
        "\n",
        "# Step 4: Exectue the SQL query\n",
        "result_df = sql_query_to_pd(sql_query_string, db_name='default.db')\n",
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFAnY8vAhPlS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "167c1561c174e91518859240caf65cbf556e74a4dcd18fddf53afddc95ecd2f9"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}